{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NarasimhanRadhakrishnan/pyspark/blob/main/setup/Pyspark_setup.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up for **pyspark**\n"
      ],
      "metadata": {
        "id": "Utd4LA_7GKxB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt update\n",
        "!sudo apt install openjdk-8-jdk-headless -qq >/dev/null"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8RAL4mfIAc4",
        "outputId": "7c32e338-24f4-4c77-8062-0bd004e06773"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rHit:1 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "\u001b[33m\r0% [Waiting for headers] [Connected to cloud.r-project.org (108.157.173.97)] [C\u001b[0m\r                                                                               \rHit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "\u001b[33m\r0% [Waiting for headers] [Connected to cloud.r-project.org (108.157.173.97)] [W\u001b[0m\r                                                                               \rHit:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "36 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Download spark-3.5.3-bin-hadoop3, a specific distribution of Apache Spark\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.5.6/spark-3.5.6-bin-hadoop3.tgz\n"
      ],
      "metadata": {
        "id": "RfXFVDD6JKVI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unpack the contents of the spark-3.5.3-bin-hadoop3.tgz file into the file system\n",
        "!tar xf spark-3.5.6-bin-hadoop3.tgz"
      ],
      "metadata": {
        "id": "itStchUNJKTt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration of environment variables\n",
        "# The JAVA_HOME environment variable points to the Java installation directory on the machine and is essential for Spark\n",
        "# The SPARK_HOME environment variable points to the Apache Spark installation directory. It is used by Spark to localize its own components and libraries.\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = f\"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/spark-3.5.6-bin-hadoop3\""
      ],
      "metadata": {
        "id": "NYHrXUr7JnLG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Installation of the required libaries\n",
        "\n",
        "pip install -q findspark\n",
        "pip install py4j -q\n",
        "pip install pyspark==3.5.1\n",
        "pip install dataproc-spark-connect==0.8.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 862
        },
        "id": "NcufvLq5K9Mn",
        "outputId": "0b4f8a02-4b6c-4845-acfd-092d422edc76"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark==3.5.6\n",
            "  Using cached pyspark-3.5.6-py2.py3-none-any.whl\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark==3.5.6) (0.10.9.7)\n",
            "Installing collected packages: pyspark\n",
            "  Attempting uninstall: pyspark\n",
            "    Found existing installation: pyspark 3.5.1\n",
            "    Uninstalling pyspark-3.5.1:\n",
            "      Successfully uninstalled pyspark-3.5.1\n",
            "Successfully installed pyspark-3.5.6\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pyspark"
                ]
              },
              "id": "1bc5cdeb64f14c23909b616f11f5062f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: dataproc-spark-connect==0.8.1 in /usr/local/lib/python3.11/dist-packages (0.8.1)\n",
            "Requirement already satisfied: google-api-core>=2.19 in /usr/local/lib/python3.11/dist-packages (from dataproc-spark-connect==0.8.1) (2.25.1)\n",
            "Requirement already satisfied: google-cloud-dataproc>=5.18 in /usr/local/lib/python3.11/dist-packages (from dataproc-spark-connect==0.8.1) (5.21.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from dataproc-spark-connect==0.8.1) (24.2)\n",
            "Requirement already satisfied: pyspark~=3.5.1 in /usr/local/lib/python3.11/dist-packages (from pyspark[connect]~=3.5.1->dataproc-spark-connect==0.8.1) (3.5.6)\n",
            "Requirement already satisfied: tqdm>=4.67 in /usr/local/lib/python3.11/dist-packages (from dataproc-spark-connect==0.8.1) (4.67.1)\n",
            "Requirement already satisfied: websockets>=14.0 in /usr/local/lib/python3.11/dist-packages (from dataproc-spark-connect==0.8.1) (15.0.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core>=2.19->dataproc-spark-connect==0.8.1) (1.70.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /usr/local/lib/python3.11/dist-packages (from google-api-core>=2.19->dataproc-spark-connect==0.8.1) (5.29.5)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core>=2.19->dataproc-spark-connect==0.8.1) (1.26.1)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core>=2.19->dataproc-spark-connect==0.8.1) (2.38.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core>=2.19->dataproc-spark-connect==0.8.1) (2.32.3)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-dataproc>=5.18->dataproc-spark-connect==0.8.1) (0.14.2)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark~=3.5.1->pyspark[connect]~=3.5.1->dataproc-spark-connect==0.8.1) (0.10.9.7)\n",
            "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.11/dist-packages (from pyspark[connect]~=3.5.1->dataproc-spark-connect==0.8.1) (2.2.2)\n",
            "Requirement already satisfied: pyarrow>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from pyspark[connect]~=3.5.1->dataproc-spark-connect==0.8.1) (18.1.0)\n",
            "Requirement already satisfied: grpcio>=1.56.0 in /usr/local/lib/python3.11/dist-packages (from pyspark[connect]~=3.5.1->dataproc-spark-connect==0.8.1) (1.73.1)\n",
            "Requirement already satisfied: grpcio-status>=1.56.0 in /usr/local/lib/python3.11/dist-packages (from pyspark[connect]~=3.5.1->dataproc-spark-connect==0.8.1) (1.71.2)\n",
            "Requirement already satisfied: numpy<2,>=1.15 in /usr/local/lib/python3.11/dist-packages (from pyspark[connect]~=3.5.1->dataproc-spark-connect==0.8.1) (1.26.4)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-api-core>=2.19->dataproc-spark-connect==0.8.1) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-api-core>=2.19->dataproc-spark-connect==0.8.1) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-api-core>=2.19->dataproc-spark-connect==0.8.1) (4.9.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.5->pyspark[connect]~=3.5.1->dataproc-spark-connect==0.8.1) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.5->pyspark[connect]~=3.5.1->dataproc-spark-connect==0.8.1) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.5->pyspark[connect]~=3.5.1->dataproc-spark-connect==0.8.1) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core>=2.19->dataproc-spark-connect==0.8.1) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core>=2.19->dataproc-spark-connect==0.8.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core>=2.19->dataproc-spark-connect==0.8.1) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core>=2.19->dataproc-spark-connect==0.8.1) (2025.6.15)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-api-core>=2.19->dataproc-spark-connect==0.8.1) (0.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.5->pyspark[connect]~=3.5.1->dataproc-spark-connect==0.8.1) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# findspark.init() makes it easy to configure and launch Apache Spark in local development environments\n",
        "import findspark\n",
        "findspark.init()\n",
        ""
      ],
      "metadata": {
        "id": "eWwsgaCbMtgZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "# Create a PySpark session\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark= (SparkSession\n",
        "        .builder\n",
        "        .appName(\"PySaprk Connection in Google Colab\")\n",
        "        .getOrCreate()\n",
        ")\n",
        "\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "vwsAiAOAMzBI",
        "outputId": "0fcea964-dadd-433c-cf1c-b218066d0e39"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7cd30c213c50>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://a65382f364fa:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.6</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>PySaprk Connection in Google Colab</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example Data\n",
        "\n",
        "from pyspark.sql import DataFrame\n",
        "\n",
        "# Definig the data\n",
        "data = [\n",
        "    (1,\"Jane\", \"Admin\"),\n",
        "    (2, \"Joe\", \"HR\"),\n",
        "    (3, \"John\", \"IT\"),\n",
        "    (4, \"Mary\", \"Legal\"),\n",
        "    (5, \"Kate\", \"IT\")\n",
        "]\n",
        "\n",
        "# Defining the columns\n",
        "columns = [\"id\", \"name\", \"dept\"]\n",
        "\n",
        "# Creation of Data Frame\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQea3PvsMy_V",
        "outputId": "9e5b52ba-4497-44d5-bea0-715efd032b45"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+-----+\n",
            "| id|name| dept|\n",
            "+---+----+-----+\n",
            "|  1|Jane|Admin|\n",
            "|  2| Joe|   HR|\n",
            "|  3|John|   IT|\n",
            "|  4|Mary|Legal|\n",
            "|  5|Kate|   IT|\n",
            "+---+----+-----+\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}